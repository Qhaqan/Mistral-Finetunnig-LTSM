#🔧 Mistral-7B Keyword Generator – Fine-Tuned LLM with LoRA (4-bit)
This project demonstrates the full pipeline of fine-tuning the mistralai/Mistral-7B-v0.1 language model using Low-Rank Adaptation (LoRA) and 4-bit quantization for efficient training on consumer GPUs like Google Colab's A100. The fine-tuned model specializes in generating unique, high-quality keywords based on given prompts.

#📌 Features
🔍 Custom Dataset: Trained on thousands of structured prompt-output pairs in JSONL format

⚙️ 4-bit Quantization: Efficient training using bitsandbytes

🔁 LoRA Integration: Memory-efficient fine-tuning with PEFT

🚀 Google Colab Compatible: Works seamlessly with Colab's A100 and L4 GPUs

💾 Drive Integration: Saves and loads fine-tuned checkpoints directly to/from Google Drive

📚 Refine Existing Models: Supports re-finetuning already fine-tuned models



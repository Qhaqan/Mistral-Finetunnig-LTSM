#ğŸ”§ Mistral-7B Keyword Generator â€“ Fine-Tuned LLM with LoRA (4-bit)
This project demonstrates the full pipeline of fine-tuning the mistralai/Mistral-7B-v0.1 language model using Low-Rank Adaptation (LoRA) and 4-bit quantization for efficient training on consumer GPUs like Google Colab's A100. The fine-tuned model specializes in generating unique, high-quality keywords based on given prompts.

#ğŸ“Œ Features
ğŸ” Custom Dataset: Trained on thousands of structured prompt-output pairs in JSONL format

âš™ï¸ 4-bit Quantization: Efficient training using bitsandbytes

ğŸ” LoRA Integration: Memory-efficient fine-tuning with PEFT

ğŸš€ Google Colab Compatible: Works seamlessly with Colab's A100 and L4 GPUs

ğŸ’¾ Drive Integration: Saves and loads fine-tuned checkpoints directly to/from Google Drive

ğŸ“š Refine Existing Models: Supports re-finetuning already fine-tuned models



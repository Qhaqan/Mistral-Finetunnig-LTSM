# -*- coding: utf-8 -*-
"""RTSM_jsonl_ready.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Pm8mSjvbOsnSQG8boyUehIuQkQ9aa8m
"""

!pip install -q --upgrade transformers >=4.41.0 peft==0.10.0 bitsandbytes accelerate pandas==2.2.2

from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

import pandas as pd
from datasets import Dataset

# Step 1: Load JSONL using pandas
df = pd.read_json("/content/finetune_keywords_dataset_5000.jsonl", lines=True)

# Step 2: Split the text column into prompt and output
df[['prompt', 'output']] = df['text'].str.split('=>', expand=True)
df['prompt'] = df['prompt'].str.strip()
df['output'] = df['output'].str.strip()
df = df.drop(columns=['text'])

# Step 3: Convert to Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# Step 4: Optional - preview first row
print(dataset[0])

# Model and dataset paths
MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.1"
DATA_PATH = "/content/finetune_keywords_dataset_5000.jsonl"  # Upload your file here

import pandas as pd
from datasets import Dataset

# Step 1: Load JSONL using pandas
df = pd.read_json("/content/finetune_keywords_dataset_5000.jsonl", lines=True)

# Step 2: Split the text column into prompt and output
df[['prompt', 'output']] = df['text'].str.split('=>', expand=True)
df['prompt'] = df['prompt'].str.strip()
df['output'] = df['output'].str.strip()
df = df.drop(columns=['text'])

# Step 3: Convert to Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# Step 4: Optional - preview first row
print(dataset[0])

from huggingface_hub import login

login("hf_UKTgIm**************lQLNTkJyxan")

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

tokenizer = AutoTokenizer.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    use_fast=False,
    trust_remote_code=True
)

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training

MODEL_NAME = "mistralai/Mistral-7B-v0.1"

# Load tokenizer and model in 4-bit mode
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, force_download=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    load_in_4bit=True,
    device_map="auto",  # No .to() needed
    trust_remote_code=True
)

# Prepare model for 4-bit training
model = prepare_model_for_kbit_training(model)

# LoRA config
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA
model = get_peft_model(model, lora_config)

from google.colab import drive
drive.mount('/content/drive')

from datasets import Dataset
import json

# Read local JSONL file manually
with open("/content/finetune_keywords_dataset_5000.jsonl", "r") as f:
    lines = [json.loads(line) for line in f]

# Create HuggingFace Dataset object
dataset = Dataset.from_list(lines)

# Split prompt and output
def split_prompt(example):
    parts = example["text"].split("=>")
    return {
        "prompt": parts[0].strip(),
        "output": parts[1].strip()
    }

dataset = dataset.map(split_prompt)

# Combine prompt and output into a single training text
def format_example(example):
    return {
        "text": f"{example['prompt']}\n{example['output']}"
    }

dataset = dataset.map(format_example)
print(dataset[0])

# Fix the tokenizer pad token issue
tokenizer.pad_token = tokenizer.eos_token

# Tokenize the dataset
def tokenize(example):
    return tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=512,
    )

tokenized_dataset = dataset.map(tokenize, batched=True)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="/content/mistral-llm",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=1.5,
    fp16=True,  # Use bf16=True if on A100 with support
    logging_steps=10,
    save_steps=200,
    save_total_limit=1,
    learning_rate=2e-4,
    report_to="none"
)

from transformers import DataCollatorForLanguageModeling

# Define the data collator for causal language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # No masked language modeling for causal models like Mistral
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer
)

trainer.train()

save_directory = "/content/drive/MyDrive/mistral-llm"

# Save model and tokenizer
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

print(f"Model saved to {save_directory}")

### load model now ####

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Path to your saved model in Google Drive
model_path = "/content/drive/MyDrive/mistral-llm"

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)


# Set the model to evaluation mode
model.eval()

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
model_path = "/content/drive/MyDrive/mistral-llm"

# Load model and tokenizer from Google Drive
model_path = model_path
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)
model.eval()

# Input prompt
prompt = "Generate keywords for Online promotion tools"

# Tokenize
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Generate output
with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=30,
        do_sample=True,
        top_p=0.95,
        temperature=0.7,
        eos_token_id=tokenizer.eos_token_id,
    )

# Decode and print the output (skipping the prompt part)
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
generated_keywords = output_text[len(prompt):].strip()

print("ðŸ”‘ Generated Keywords:\n", generated_keywords)